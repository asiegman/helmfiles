#####
# WARNING: This is a complete rewrite of this release as we switch
# to the current helm stable chart at https://github.com/helm/charts/tree/master/stable/prometheus-operator
# from the deprecated coreos-stable chart at https://github.com/coreos/prometheus-operator/tree/master/helm/prometheus-operator
#
# OLD References:
# OLD  - https://github.com/coreos/prometheus-operator/tree/master/helm/prometheus-operator
# OLD  - https://github.com/coreos/prometheus-operator
#
repositories:
  # Official helm charts
  - name: "stable"
    url: "https://kubernetes-charts.storage.googleapis.com/"

releases:

  #######################################################################################
  ## prometheus-operator                                                               ##
  ## creates/configures/manages Prometheus clusters atop Kubernetes                    ##
  ## This is the all-in-one version that                                               ##
  ## replaces https://github.com/coreos/prometheus-operator/tree/master/helm           ##
  #######################################################################################

  #
  # References:
  #   - https://github.com/helm/charts/tree/master/stable/prometheus-operator
  #   - https://github.com/coreos/prometheus-operator
  #
  - name: "prometheus-operator"
    namespace: "monitoring"
    labels:
      chart: "prometheus-operator"
      repo: "coreos-stable"
      component: "monitoring"
      namespace: "monitoring"
      vendor: "coreos"
      default: "true"
    chart: "stable/prometheus-operator"
    # https://github.com/helm/charts/tree/50fa3099e2f460f3aba04870b845b1b076a5ed8a/stable/prometheus-operator
    version: "6.2.1"
    wait: true
    installed: {{ env "PROMETHEUS_OPERATOR_INSTALLED" | default "true" }}
    hooks:
    # Hooks associated to presync events are triggered before each release is applied to the remote cluster.
    # This is the ideal event to execute any commands that may mutate the cluster state as it
    # will not be run for read-only operations like lint, diff or template.
    # These hoook install the prometheuses.monitoring.coreos.com CustomResourceDefinition if needed
    - events: ["presync"]
      command: "/bin/sh"
      args: ["-c", "kubectl get crd prometheuses.monitoring.coreos.com >/dev/null 2>&1 || \
             kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheus.crd.yaml"]
    # This hoook installs the alertmanagers.monitoring.coreos.com CustomResourceDefinition if needed
    - events: ["presync"]
      command: "/bin/sh"
      args: ["-c", "kubectl get crd alertmanagers.monitoring.coreos.com >/dev/null 2>&1 || \
             kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/alertmanager.crd.yaml"]
    # This hoook installs the prometheusrules.monitoring.coreos.com CustomResourceDefinition if needed
    - events: ["presync"]
      command: "/bin/sh"
      args: ["-c", "kubectl get crd prometheusrules.monitoring.coreos.com >/dev/null 2>&1 || \
             kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheusrule.crd.yaml"]
    # This hoook installs the servicemonitors.monitoring.coreos.com CustomResourceDefinition if needed
    - events: ["presync"]
      command: "/bin/sh"
      args: ["-c", "kubectl get crd servicemonitors.monitoring.coreos.com >/dev/null 2>&1 || \
             kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/servicemonitor.crd.yaml"]
    values:
    - global:
        rbac:
          create: {{ env "RBAC_ENABLED" | default "true" }}
          pspEnabled: {{ coalesce (env "POD_SECURITY_POLICY_ENALBED") (env "RBAC_ENABLED") "true" }}
      defaultRules:
        create: true
      additionalPrometheusRules: []
      prometheusOperator:
        enabled: {{ env "PROMETHEUS_OPERATOR_INSTALLED" | default "true" }}
        ### Create CRDs separately
        createCustomResource: false
        # log level must be one of "all", "debug",	"info", "warn",	"error", "none"
        logLevel: "warn"
        resources:
          limits:
            cpu: '{{ env "PROMETHEUS_OPERATOR_LIMIT_CPU" | default "100m" }}'
            memory: '{{ env "PROMETHEUS_OPERATOR_LIMIT_MEMORY" | default "96Mi" }}'
          requests:
            cpu: '{{ env "PROMETHEUS_OPERATOR_REQUEST_CPU" | default "20m" }}'
            memory: '{{ env "PROMETHEUS_OPERATOR_REQUEST_MEMORY" | default "48Mi" }}'
        image:
          pullPolicy: "IfNotPresent"
      prometheus:
        enabled: {{ env "PROMETHEUS_OPERATOR_INSTALLED" | default "true" }}
        podDisruptionBudget:
          enabled: false
        ingress:
          enabled: false
        additionalServiceMonitors: []
        prometheusSpec:
          {{- if or (env "PROMETHEUS_OPERATOR_PROMETHEUS_IMAGE_REPOSITORY") (env "PROMETHEUS_OPERATOR_PROMETHEUS_IMAGE_TAG") }}
          image:
            {{- if env "PROMETHEUS_OPERATOR_PROMETHEUS_IMAGE_REPOSITORY" }}
            repository: {{ env "PROMETHEUS_OPERATOR_PROMETHEUS_IMAGE_REPOSITORY" }}
            {{- end }}
            {{- if env "PROMETHEUS_OPERATOR_PROMETHEUS_IMAGE_TAG" }}
            tag: {{ env "PROMETHEUS_OPERATOR_PROMETHEUS_IMAGE_TAG" }}
            {{- end }}
          {{- end }}
          replicas: 1
          retention: {{ env "PROMETHEUS_OPERATOR_PROMETHEUS_RETENTION_PERIOD" | default "45d" }}
          logLevel: "warn"
          scrapeInterval: ""
          evaluationInterval: ""
          ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
          ## prometheus resource to be created with selectors based on values in the helm deployment,
          ## which will also match the PrometheusRule resources created.
          ## If false, a nil or or {} value for ruleSelector will select all PrometheusRule resources.
          ruleSelectorNilUsesHelmValues: false
          ## serviceMonitorSelectorNilUsesHelmValues works just like ruleSelectorNilUsesHelmValues
          serviceMonitorSelectorNilUsesHelmValues: false
          {{- if env "PROMETHEUS_OPERATOR_PROMETHEUS_STORAGE_SIZE" }}
          storageSpec:
            volumeClaimTemplate:
              spec:
                {{- if env "PROMETHEUS_OPERATOR_PROMETHEUS_STORAGE_CLASS" }}
                storageClassName: {{ env "PROMETHEUS_OPERATOR_PROMETHEUS_STORAGE_CLASS" }}
                {{- end }}
                resources:
                  requests:
                    storage: {{ env "PROMETHEUS_OPERATOR_PROMETHEUS_STORAGE_SIZE" }}
          {{- end }}
          externalUrl: "{{- env "PROMETHEUS_PROMETHEUS_EXTERNAL_URL" | default (print "https://api." (env "KOPS_CLUSTER_NAME") "/api/v1/namespaces/monitoring/services/prometheus-operator-prometheus:web/proxy/") }}"
          resources:
            limits:
              cpu: '{{ env "PROMETHEUS_PROMETHEUS_LIMIT_CPU" | default "300m" }}'
              memory: '{{ env "PROMETHEUS_PROMETHEUS_LIMIT_MEMORY" | default "1Gi" }}'
            requests:
              cpu: '{{ env "PROMETHEUS_PROMETHEUS_REQUEST_CPU" | default "50m" }}'
              memory: '{{ env "PROMETHEUS_PROMETHEUS_REQUEST_MEMORY" | default "512Mi" }}'
      alertmanager:
        enabled: {{ env "ALERTMANAGER_INSTALLED" | default "true" }}
        ## Alertmanager configuration directives
        ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
        ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
        ##
        config:
          global:
            resolve_timeout: 5m
          route:
            group_by:
              - "alertname"
              - "namespace"
            group_wait: 30s
            group_interval: 5m
            repeat_interval: 12h
            receiver: 'general'
            routes:
              - match:
                  alertname: Watchdog
                receiver: 'null'
          receivers:
            - name: 'null'
            - name: "general"
{{ if not ( env "KUBE_PROMETHEUS_ALERT_MANAGER_SLACK_WEBHOOK_URL" | empty ) }}
              slack_configs:
                ### Required: KUBE_PROMETHEUS_ALERT_MANAGER_SLACK_WEBHOOK_URL
                - api_url: '{{ env "KUBE_PROMETHEUS_ALERT_MANAGER_SLACK_WEBHOOK_URL" }}'
                  ### Required: KUBE_PROMETHEUS_ALERT_MANAGER_SLACK_CHANNEL; e.g. #alerts-staging
                  channel: '{{ env "KUBE_PROMETHEUS_ALERT_MANAGER_SLACK_CHANNEL" }}'
                  send_resolved: true
{{ end }}
{{ if not ( env "KUBE_PROMETHEUS_ALERT_MANAGER_PAGERDUTY_INTEGRATION_KEY" | empty ) }}
              pagerduty_configs:
                - routing_key: '{{ env "KUBE_PROMETHEUS_ALERT_MANAGER_PAGERDUTY_INTEGRATION_KEY" }}'
                  send_resolved: true
                  details:
                    firing: '{{"{{"}} template "pagerduty.default.firing" . {{"}}"}}'
                    resolved: '{{"{{"}} template "pagerduty.default.resolved" . {{"}}"}}'
{{ end }}
          templates:
            - ./*.tmpl
        alertmanagerSpec:
          externalUrl: "{{- env "PROMETHEUS_ALERTMANAGER_EXTERNAL_URL" | default (print "https://api." (env "KOPS_CLUSTER_NAME") "/api/v1/namespaces/monitoring/services/prometheus-operator-alertmanager:web/proxy/") }}"
          resources:
            limits:
              cpu: '{{ env "PROMETHEUS_ALERTMANAGER_LIMIT_CPU" | default "200m" }}'
              memory: '{{ env "PROMETHEUS_ALERTMANAGER_LIMIT_MEMORY" | default "96Mi" }}'
            requests:
              cpu: '{{ env "PROMETHEUS_ALERTMANAGER_REQUEST_CPU" | default "10m" }}'
              memory: '{{ env "PROMETHEUS_ALERTMANAGER_REQUEST_MEMORY" | default "24Mi" }}'
      grafana:
        enabled: {{ env "GRAFANA_INSTALLED" | default "true" }}
        adminPassword: {{ env "GRAFANA_ADMIN_PASSWORD" | default "prom-operator-CHANGEME" }}
        defaultDashboardsEnabled: true
        sidecar:
          dashboards:
            enabled: true
            searchNamespace: ALL
            defaultFolderName: "AutoLoaded"
        plugins:
          - grafana-piechart-panel
        grafana.ini:
          server:
            # root_url: "https://api.{{- env "KOPS_CLUSTER_NAME" }}/api/v1/namespaces/kube-system/services/prometheus-operator-grafana:service/proxy/"
            root_url: "{{- env "PROMETHEUS_GRAFANA_ROOT_URL" | default (print "https://api." (env "KOPS_CLUSTER_NAME") "/api/v1/namespaces/monitoring/services/prometheus-operator-grafana:service/proxy/") }}"
          {{- if eq (env "PROMETHEUS_ANONYMOUS_ADMIN_ENABLED" | default "true") "true" }}
          auth.anonymous:
            enabled: true
            org_role: Admin
          {{- end }}
          {{- if eq (env "PROMETHEUS_KEYCLOAK_GATEKEEPER_ENABLED" | default "false") "true" }}
          auth:
            oauth_auto_login: true
            disable_signout_menu: true
          users:
            auto_assign_org: true
            auto_assign_org_id: 1
            auto_assign_org_role: '{{ env "PROMETHEUS_KEYCLOAK_GATEKEEPER_AUTO_ROLE" | default "Admin" }}'
          auth.generic_oauth:
            enabled: true
            allow_sign_up:  true
            scopes:  openid profile email
            client_id:  '{{- coalesce (env "KEYCLOAK_GATEKEEPER_CLIENT_ID") (env "KOPS_OIDC_CLIENT_ID") "kubernetes" }}'
            client_secret:  '{{- requiredEnv "KEYCLOAK_GATEKEEPER_CLIENT_SECRET" }}'
            auth_url:  '{{- coalesce (env "KEYCLOAK_GATEKEEPER_DISCOVERY_URL") (env "KOPS_OIDC_ISSUER_URL") }}/protocol/openid-connect/auth'
            token_url:  '{{- coalesce (env "KEYCLOAK_GATEKEEPER_DISCOVERY_URL") (env "KOPS_OIDC_ISSUER_URL") }}/protocol/openid-connect/token'
            api_url:  '{{- coalesce (env "KEYCLOAK_GATEKEEPER_DISCOVERY_URL") (env "KOPS_OIDC_ISSUER_URL") }}/protocol/openid-connect/userinfo'
          {{- end }}
          {{- if env "GRAFANA_DB_HOST" }}
          # We are only supporting MySQL at this time
          database:
            type: mysql
            host: '{{- env "GRAFANA_DB_HOST" }}:{{- env "GRAFANA_DB_PORT" | default "3306" }}'
            name: {{ env "GRAFANA_DB_NAME" | default "grafana" }}
            user: {{ env "GRAFANA_DB_USER" | default "grafana" }}
            password: '"""{{ env "GRAFANA_DB_PASSWORD" }}"""'
            ssl_mode: skip-verify
            # Should not need this, but Grafana requires some certificate be
            # present, even with ssl_mode: skip-verify
            ca_cert_path: /etc/ssl/certs/Amazon_Root_CA_4.pem
            log_queries: {{ env "GRAFANA_LOG_QUERIES" | default "false" }}
          {{- end }}
      kubeStateMetrics:
        enabled: true
      {{- if or (env "PROMETHEUS_OPERATOR_KSM_IMAGE_REPOSITORY") (env "PROMETHEUS_OPERATOR_KSM_IMAGE_TAG") }}
      kube-state-metrics:
        image:
          {{- if env "PROMETHEUS_OPERATOR_KSM_IMAGE_REPOSITORY" }}
          repository: {{ env "PROMETHEUS_OPERATOR_KSM_IMAGE_REPOSITORY" }}
          {{- end }}
          {{- if env "PROMETHEUS_OPERATOR_KSM_IMAGE_TAG" }}
          tag: {{ env "PROMETHEUS_OPERATOR_KSM_IMAGE_TAG" }}
          {{- end }}
      {{- end }}
      kubeApiServer:
        enabled: true
      kubelet:
        enabled: true
        # In general, few clusters are set up to allow kublet to authenticate a bearer token, and
        # the HTTPS endpoint requires authentication, so Prometheus cannot access it.
        # The HTTP endpoint does not require authentication, so Prometheus can access it.
        # See https://github.com/coreos/prometheus-operator/issues/926
        serviceMonitor:
          https: {{ env "PROMETHEUS_KUBELET_HTTPS" | default "false" }}
      kubeControllerManager:
        enabled: true
        {{- if eq (env "PROMETHEUS_KUBE_K8S_APP_LABEL_NAME" | default "false") "true" }}
        service:
          selector:
            k8s-app: "kube-controller-manager"
            component: null
        {{- end }}
      coreDns:
        enabled: {{ coalesce (env "PROMETHEUS_EXPORTER_CORE_DNS_ENABLED") "true" }}
      kubeDns:
        enabled: {{ coalesce (env "PROMETHEUS_EXPORTER_KUBE_DNS_ENABLED") "false" }}
      kubeEtcd:
        # Access to etcd is a huge security risk, so nodes are blocked from accessing it.
        # Therefore Prometheus cannot access it without extra setup, which is beyond the scope of this helmfile.
        # See https://github.com/kubernetes/kops/issues/5852
        #     https://github.com/kubernetes/kops/issues/4975#issuecomment-381055946
        #     https://github.com/coreos/prometheus-operator/issues/2397
        #     https://github.com/coreos/prometheus-operator/blob/v0.19.0/contrib/kube-prometheus/docs/Monitoring%20external%20etcd.md
        #     https://gist.github.com/jhohertz/476bd616d4171649a794b8c409f8d548
        # So we disable it since it is not going to work anyway
        enabled: false
      kubeScheduler:
        enabled: true
        {{- if eq (env "PROMETHEUS_KUBE_K8S_APP_LABEL_NAME" | default "false") "true" }}
        service:
          selector:
            k8s-app: "kube-scheduler"
            component: null
        {{- end }}
      nodeExporter:
        enabled: true
    {{- if env "PROMETHEUS_ADDITIONAL_CONFIG_YAML" }}
    - {{ env "PROMETHEUS_ADDITIONAL_CONFIG_YAML" }}
    {{ end }}
    set:
    - name: "alertmanager.templateFiles.deployment\\.tmpl"
      file: ./values/kube-prometheus.alerts.template
